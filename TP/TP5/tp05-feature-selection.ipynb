{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7e4d06",
   "metadata": {},
   "source": [
    "# TP n°4 - Feature selection\n",
    "**Objectif:**\n",
    "\n",
    "Ce TP vise à étudier quelques méthodes de sélection de variables (Filtrage) en utilisant la bibliothèque Scikit-learn.\n",
    "\n",
    "[Documentation scikit-learn -> feature selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "\n",
    "Durant ce TP, on va aussi utiliser un modèle de réduction de variables (AFD).\n",
    "Dans Scikit-learn, l'analyse factorielle discriminante (AFD) est mise en oeuvre dans la classe [LinearDiscriminantAnalysis](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9174a96d",
   "metadata": {},
   "source": [
    "# Scikit-learn\n",
    "Scikit-learn ou sklearn est une bibliothèque python libre conçue pour effectuer de l'apprentissage automatique.\n",
    "Elle propose un set d'algorithmes de classification, régression et regroupement etc.\n",
    "\n",
    "# Dataset\n",
    "Pour illustrer le propos, on va utiliser le dataset `sales.csv` et nous allons ajouter des variables aléatoires qui représentent du bruit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Chargement du dataset sales.csv\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cefc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les dimension du dataset\n",
    "# TODO\n",
    "\n",
    "# Afficher les informations du dataset\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e6e87",
   "metadata": {},
   "source": [
    "Pour l'exemple, on va supposer que la variable `division` (variable expliquée) contient les étiquettes des classes.\n",
    "\n",
    "Pour référence, appliquons l'étape décisionnelle de l'analyse discriminante (comme modèle décisionnel) sur les données initiales et ensuite sur les données auxquelles les nouvelles variables aléatoires ont été ajoutées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098271e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des subsets d'entrainement et de test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encoding des variables catégorielles\n",
    "# TODO\n",
    "\n",
    "# Ajout de 2 variables pour bruiter\n",
    "np.random.seed(42)\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df.drop(['level of education', 'education labels', 'division'], axis=1) # variables explicatives\n",
    "y = df['education labels'] # variable expliquée\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state=42)\n",
    "print(f\"dimension du subset d'entrainement: {X_train.shape}\")\n",
    "print(f\"dimension du subset de test: {X_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train[0:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Création du modèle\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# entrainement avec les données initiales\n",
    "# TODO\n",
    "\n",
    "# entrainement avec les données bruitées\n",
    "# TODO\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "75b06824",
   "metadata": {},
   "source": [
    "### Question\n",
    "Comparer les performances du modèle entrainé sur les données initiales puis sur les données bruitées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correction\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "d064e07e",
   "metadata": {},
   "source": [
    "# Filtrage\n",
    "La famille de filtrage est appliquée sans faire appel à un modèle prédictif.\n",
    "Elle permet de maximiser l'information mutuelle entre la variable d'entrée et celle de sortie.\n",
    "Elle minimise la redondance entre les variables d'entrée.\n",
    "NB: Une coopération sous optimale avec le modèle prédictif qui n'intervient pas dans la sélection.\n",
    "Cette approche présente un coût inférieur à celui de l'approche Wrapper.\n",
    "\n",
    "## Suppression de variables à faible variance\n",
    "La suppression de variables à faible variance consiste à éliminer les variables dont la variance est inférieure à un seuil (par défaut la valeur du seuil est 0, sont donc éliminées les variables constantes).\n",
    "[Voir plus](https://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Proposer une suppression de variables en utilisant le code ci-dessous"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Exemple de filtrage avec la suppression de variables à faible variance\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe49b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# affichage des variances des différentes variables\n",
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fixer un seuil de variance à 10 pour éliminer quelques variables\n",
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# affichage des variances des différentes variables\n",
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Affichage des variables sélectionnées\n",
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "41dac6be",
   "metadata": {},
   "source": [
    "### Question\n",
    "Commenter la variance des variables ajoutées (pour `threshold=10`)\n",
    "A travers une autre combinaison de variables, proposer une 2ème classification via le même modèle étudié `from sklearn.discriminant_analysis import LinearDiscriminantAnalysis`.\n",
    "Commenter le résultat obtenu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72424a5f",
   "metadata": {},
   "source": [
    "### Correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e70536",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearDiscriminantAnalysis()\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "1c41a56d",
   "metadata": {},
   "source": [
    "## Sélection univariée\n",
    "La sélection univariées cherche à déterminer (à travers des tests statistiques comme le test de `Chi2`) dans quelle mesure chaque variable d'entrée \"explique\" la variable de sortie; les variables les moins explicatives individuellement sont éliminées.\n",
    "\n",
    "Cette méthode élimine les variables pour lesquelles les valeurs de l'information mutuelle avec la variable de sortie sont les plus faibles (c'est à dire, qui « expliquent » le moins bien la variable de sortie). Nous avons l'intention de garder la moitié des variables et utilisons donc la fonction `SelectKBest` (d'autres sont disponibles, voir la documentation).\n",
    "\n",
    "[Doc. sélection univariée](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)\n",
    "[Doc. information mutuelle pour la classification](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question\n",
    "Utilisez la sélection univariée pour sélectionner les meilleurs groupe de variables qui explique le mieux la variable expliquée."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Exemple de filtrage avec la sélection univariée\n",
    "from sklearn.feature_selection import SelectKBest, f_oneway\n",
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "34db644c",
   "metadata": {},
   "source": [
    "### Question\n",
    "Que constatez-vous par rapport aux variables ajoutées (bruit) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "0c20c1dc",
   "metadata": {},
   "source": [
    "### Question\n",
    "Décrire la performance du modèle décisionnel. Comparer aux résultats obtenus par la méthode précédente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e85ddbe",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c386c1",
   "metadata": {},
   "source": [
    "# Wrapper\n",
    "La famille Wrapper coopère directement avec le modèle prédictif.\n",
    "Choix des variables qui maximisent les performances du modèle.\n",
    "**Inconvénients :**\n",
    "- Coût élevé\n",
    "- Pas de justification théorique de la sélection des variables\n",
    "- Incompréhension des relations de dépendances entre les variables.\n",
    "- La procédure de sélection est spécifique au modèle utilisé.\n",
    "\n",
    "## Sélection séquentielle\n",
    "Un modèle décisionnel doit être développé sur chaque (sous-)ensemble candidat de variables d'entrée et c'est la performance du modèle qui caractérise le (sous-)ensemble de variable. Deux versions sont proposées, une incrémentale (*Forward-SFS*) et une décrémentale (*Backward-SFS*).\n",
    "[Voir plus](https://scikit-learn.org/stable/modules/feature_selection.html#sequential-feature-selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ccdf04",
   "metadata": {},
   "source": [
    "## Elimination récursive de variables\n",
    "**Principe:** apprentissage du modèle sur la totalité des variables d'entrée pour extraire la pertinence de chaque variable (variance).\n",
    "[Voir plus](https://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination)\n",
    "\n",
    "## Sélection avec SelectFromModel\n",
    "**Principe:** apprentissage du modèle sur la totalité des variables d'entrée pour extraire la pertinence de chaque variable (variance).\n",
    "[Voir plus](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-using-selectfrommodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c0813",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "L'embedding (ou l'intégration) est une opération de sélection qui est intégrée à la méthode de construction de modèle. Pas de sur-coût par rapport à la construction du modèle mais cette approche ne peut pas être utilisée avec tout type de modèle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercice\n",
    "Quelles sont les meilleures caractéristiques du dataset `sales.csv` qui permettent de prédire au mieux le salaire ?\n",
    "**Note:** Vous pouvez utiliser ce modèle: `from sklearn.linear_model import LinearRegression`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TP suivant ?\n",
    "TP n°6: La réduction des variables par réduction des dimensions --> ACP.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "date": 1.6468449935806804E9,
  "filename": "tpSelectionVariables.rst",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "title": "Travaux pratiques - Sélection de variables"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
